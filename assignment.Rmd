---
title: "MachineLearning"
author: "Krishna Teja"
date: "May 9, 2018"
output: html_document
---

#Introduction
This report is prepared as one of the requirement in Practical Machine Learning online course by Johns Hopkins University. The basic goal of this assignment is to predict the manner of the subject (6 participants) performed some exercise. For this assignment, in order to predict the manner of the subject did the exercise decision tree and random forest method will be performed to determine the best prediction. The best prediction is determined by the highest accuracy.

#Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement ??? a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here
##Load the required Packages
```{r}

library("caret")
library("randomForest")
library("rpart.plot")

```


#Loading the test and training data
```{r}

download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv" ,destfile = "pml.training.csv")
download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv" ,destfile = "pml.testing.csv")

testing<-read.csv("pml-testing.csv",na.strings = c("NA", "#DIV/0!", ""))
training<-read.csv("pml-training.csv",na.strings = c("NA", "#DIV/0!", ""))

training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]

# Delete variables that are not related 
training <- training[, -c(1:7)]
testing <- testing[, -c(1:7)]

```

##Partition the training and test data sets.
```{r }
trainingPartitionData<-createDataPartition(training$classe,p=0.7,list=F)
trainingDataSet<-training[trainingPartitionData,]
testingDataSet<-training[-trainingPartitionData,]
dim(trainingDataSet)
dim(testingDataSet)

```

#Prediction Model 1 -Decision Tree
```{r }

decisionTreeModel <- rpart(classe ~ ., data = trainingDataSet, method = "class")
decisionTreePrediction <- predict(decisionTreeModel, testingDataSet, type = "class")

# Plot Decision Tree
rpart.plot(decisionTreeModel, main = "Decision Tree", under = T,faclen = 0)
```

Compare the results and the accuracy using confusion matrix.

```{r}

confusionMatrix(decisionTreePrediction,testingDataSet$classe)

```

#Prediction Model 2- Random forest

```{r }
randomForestModel <- randomForest(classe ~. , data = trainingDataSet, method = "class")
predictionModel<-predict(randomForestModel,testingDataSet,type="class")

##result using confusoin matrix
confusionMatrix(predictionModel,testingDataSet$classe)

```

#Conclusion
It is clear that random forest is better than decision tree model since the accuray is that way (0.9915 > 0.6644)

So the final model that will be used is random forest model.
```{r}

finalModel<-predict(randomForestModel,testingDataSet,type="class")
```